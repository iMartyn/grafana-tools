#!/usr/bin/python -tt
#
import datetime
import json
import logging
import logging.handlers
import optparse
import os
import codecs
import re
import shutil
import sys
import tarfile
import tempfile
import urllib
import urllib2


def grafana_dashboard_backups(elastic_host, elastic_port, dest_dir, debug=False):
  '''Saves all Grafana dashboards stored in Elasticsearch in a tarball
  
     Uses Elasticsearch API to get all saved dashboards into a tarball, the 
     tarball contains one file per dashboard and is available in destination dir
     which is parameterized (defaults to /var/opt/grafana-dashboards-backups).'''
  
  #Set Logging to syslog
  try:
    logger = logging.getLogger(__name__)
    if debug:
      logger.setLevel(logging.DEBUG)
    else:
      logger.setLevel(logging.INFO)
  
    formatter = logging.Formatter('%(pathname)s: %(message)s')
  
    syslog_handler = logging.handlers.SysLogHandler(address = '/dev/log')
    syslog_handler.setFormatter(formatter)
    logger.addHandler(syslog_handler)
  except Exception:
    logging.info('Could not set syslog logging handler')


  if debug:
    urllib2.install_opener(urllib2.build_opener(urllib2.HTTPHandler(debuglevel=1)))


  def search(url, params):
    search_url = '%s?%s' %  (url, urllib.urlencode(params))
    try:
      response = urllib2.urlopen(search_url, timeout=5)
    except urllib2.URLError as e:
      if debug:
        logger.critical('Failed accessing: %s' % search_url)
      logger.critical(e)
      sys.exit(1)

    return json.load(response)

  def scan():
    return search(dashboards_url, {'search_type': 'scan', 'scroll': scroll_time})

  def scroll(scroll_id):
    url = '%s/%s' % (scroll_url, scroll_id)
    return search(url, {'scroll': scroll_time})
  
  def cam_case_convert(name):
    '''strips spaces and replace CamCasing.cam with cam_casing_cam'''
  
    s1 = re.sub('([^._])([A-Z][a-z]+)', r'\1_\2', name.replace(' ',''))
    s1 = s1.replace('.','_')
    return re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()

  def json_dump(data,filename):
    dump_file = codecs.open(filename, 'w', 'utf-8')
    json.dump(data, dump_file, sort_keys = True, indent = 2, ensure_ascii=False)
    dump_file.close()
  
  # Conveniance vars
  es_index_name  = 'grafana-dash'
  es_type        = 'dashboard'
  es_url         = 'http://%s:%s' % (elastic_host, elastic_port)
  dashboards_url = '%s/%s/%s/_search' % (es_url, es_index_name, es_type)
  scroll_url     = '%s/_search/scroll' % es_url
  scroll_time    = '1m'
  work_tmp_dir   = tempfile.mkdtemp()
  utc_datetime   = datetime.datetime.utcnow()
  formatted_time = utc_datetime.strftime("%Y-%m-%d-%H%MZ")
  
  if debug:
    logger.info('Grabbing grafana dashboards from: %s' % dashboards_url)

  scroll_id = scan()['_scroll_id']

  # Create a tarball with all the dashboards and move to target dir
  
  tarball = os.path.join(work_tmp_dir, 'grafana_dashboards_backup_' +
      formatted_time + '.tar.gz')
  tar = tarfile.open(tarball, 'w:gz')

  if not scroll_id:
    raise Exception('Failed to get scroll id')

  while 1:
    data      = scroll(scroll_id)
    hit_count = len(data['hits']['hits'])
    scroll_id = data['_scroll_id']
    if hit_count == 0:
      break

    try:
      for hit in data['hits']['hits']:
        source               = hit['_source']
        did                  = hit['_id']
        dashboard_definition = json.loads(source['dashboard'])
        dashboard_base_name  = cam_case_convert(did)

        dashboard_file_name  = '%s.json' % dashboard_base_name
        dashboard_file_path  = os.path.join(work_tmp_dir, dashboard_file_name)

        json_dump(dashboard_definition, dashboard_file_path)
        tar.add(dashboard_file_path, '%s/%s' % (es_index_name, dashboard_file_name))

        metadata_file_name = '%s.metadata.json' % dashboard_base_name
        metadata_file_path = os.path.join(work_tmp_dir, metadata_file_name)
        source['dashboard'] = 'stored in alternate file %s' % dashboard_file_name
        json_dump(hit, metadata_file_path)
        tar.add(metadata_file_path, '%s/%s' % (es_index_name, metadata_file_name))

        logger.info('Added %s to the dashboards backup tarball' % did)

    except Exception as e:
      logging.critical(e)
      sys.exit(1)
  
  try:
    tar.close()
    tarball_file = os.path.basename(tarball)
    tarball_dest = os.path.join(dest_dir, tarball_file)
    os.rename(tarball,tarball_dest)
    logger.info('New grafana dashboards backup at %s' % tarball_dest)
  except Exception as e:
    logging.critical('Failed to move tarball to %s' % dest_dir)
    logging.critical(e)
    sys.exit(1)
  
  # Clean up
  try:
    shutil.rmtree(work_tmp_dir)
  except Exception as e:
    logging.critical(e)
    sys.exit(1)

def main():
  parser = optparse.OptionParser()
  
  parser.add_option('-D', '--debug',
        action  = 'store_true',
        default = False,
        dest    = 'debug',
        help    = 'Debug output (very noisy)')
  
  parser.add_option('-e', '--grafana-elasticsearch-host',
    dest    = 'elastic_host',
    help    = 'The elastic search host FQDN used by grafana',
    metavar = 'ELASTIC_SEARCH_HOST')
  
  parser.add_option('-p', '--grafana-elasticsearch-port',
    default = '9200',
    dest    = 'elastic_port',
    help    = 'The elastic search port used by grafana',
    metavar = 'ELASTIC_SEARCH_PORT')
  
  parser.add_option('-t', '--dest-dir',
    default = '/var/opt/grafana-dashboards-backups',
    dest    = 'dest_dir',
    help    = 'The destination directory where dashboards will be saved',
    metavar = 'DEST_DIR')
  
  (options, args) = parser.parse_args()
  
  if not options.elastic_host:
    parser.error('An elastic search host is required')
  
  elastic_host = options.elastic_host
  elastic_port = options.elastic_port
  dest_dir = options.dest_dir
  debug = options.debug

  grafana_dashboard_backups(elastic_host, elastic_port, dest_dir, debug)

if __name__ == '__main__':
  main()
